{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wahlpflichtfach Künstliche Intelligenz II: Praktikum \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Neuronale Netze mit Tensorflow\n",
    "\n",
    "Nachdem wir uns gerade angeguckt haben, was Tensorflow grundlegen ist und das Arbeiten mit Daten aufgefrischt haben, wollen wir uns als Nächstes den Grundlagen der Neuronalen Netze widmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronen und Layer\n",
    "\n",
    "Zuerst schauen wir uns dabei ein Neuron sowie den Aufbau eines Layers an. Wobei ein Layer aus einen, bis mehreren Neuronen besteht.\n",
    "Ein Neuron besteht dabei aus drei Funktionen:\n",
    "* Übertragungsfunktion: Summe alle gewichteten Inputs addiert um den Bias-Wert\n",
    "* Aktivierungsfunktion: z. B. ReLU\n",
    "* Ausgabefunktion: Ausgabe Funktion in der Regel Identität\n",
    "\n",
    "### lineare Layer\n",
    "Jedes neuronale Netz besteht aus mehreren Schichten. Die Schichten sind also die Bausteine. \n",
    "Unsere erste Layer ist eine lineare Layer, die nur den Drive berechntet. Wir verwenden hier keine Aktivierungsfunktion.\n",
    "\n",
    "Um eine Layer zu definieren, brauchen wir eine Klasse, die von `tf.keras.layers.Layer` erbt.\n",
    "Zusätzlich enthält TensorFlow 2 bereits viele eingebaute Schichten, die Ihr hier finden könnt: https://www.tensorflow.org/api_docs/python/tf/keras/layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "\n",
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, units):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "                        shape=(input_shape[-1], self.units),\n",
    "                        initializer=tf.random_normal_initializer(),\n",
    "                        trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "                        shape=(self.units,),\n",
    "                        initializer=tf.random_normal_initializer(),\n",
    "                        trainable=True\n",
    "        )\n",
    "     \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test des linearen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = Linear(4)\n",
    "\n",
    "x = tf.ones((1,4))\n",
    "y = linear_layer(x)\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mehrere Layer verbinden\n",
    "Um mehrere Layer miteinander zu verknüpfen, können wir diese in einer neuen Klasse, die wiederum ein Layer ist kombinieren.\n",
    "Dabei sprechen wir von einem Dens Layer, wenn alle Neuronen aus der vorherigen Layer mit allen Neuronen des aktuellen Layers verknüpft sind.\n",
    "\n",
    "Arten von Layern\n",
    "* Input: Erste schicht im Netz\n",
    "* Hidden: Zwischen schichten im Netz\n",
    "* Output: Ausgabe Schicht im Netz\n",
    "\n",
    "Hier findet ihr eine anschauliche [Visualisierung](https://playground.tensorflow.org) des Prinzipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = Linear(512)\n",
    "        self.output_layer = Linear(1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.hidden_layer(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training des neuronalen Netzes\n",
    "Um das neuronale Netz zu trainieren, müssen wir zuerst noch die Loss-Funktion ([Dokumentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses)) und den Optimierer ([Dokumentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)) bestimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zum Training verwenden wir den Datensatz aus dem ersten Kapitel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.002*(x**3-x**2+2*x)+0.3\n",
    "\n",
    "xs = np.linspace(-5,5, 20, dtype=np.float32)\n",
    "ys = f(xs)\n",
    "\n",
    "ids = np.arange(len(xs))\n",
    "training_data_ids = np.random.choice(ids,15, replace=False)\n",
    "test_data_ids = ~np.isin(ids, training_data_ids)\n",
    "training_data_xs = xs[training_data_ids]\n",
    "training_data_ys = ys[training_data_ids]\n",
    "test_data_xs = xs[test_data_ids]\n",
    "test_data_ys = ys[test_data_ids]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((training_data_xs, training_data_ys)).batch(15)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data_xs, test_data_ys)).batch(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann müssen wir nur noch unsere Trainingsschleife bauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(1000):\n",
    "    epochs.append(epoch)\n",
    "    \n",
    "    for (x,y) in train_dataset:\n",
    "        \n",
    "        x = tf.reshape(x, shape=(-1,1))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            output = mlp(x)\n",
    "            loss = mse(y, output)\n",
    "            train_losses.append(loss)\n",
    "            gradients = tape.gradient(loss, mlp.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, mlp.trainable_variables))        \n",
    "\n",
    "    for (x,y) in test_dataset:\n",
    "        x = tf.reshape(x, shape=(-1,1))\n",
    "        output = mlp(x)\n",
    "        loss = mse(y, output)       \n",
    "        test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend können wir mit unseren gesammelten Werten den Traingsprozess visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(epochs, train_losses)\n",
    "plt.plot(epochs, np.array(test_losses))\n",
    "plt.legend((\"train\",\"test\"))\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und zum Schluss können wir auch noch die Approximation visualisieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(training_data_xs, training_data_ys, c='red')\n",
    "plt.scatter(test_data_xs, test_data_ys, c='blue')\n",
    "\n",
    "xs = np.linspace(-5,5,100, dtype=np.float32)\n",
    "xs = np.reshape(xs, newshape=(-1,1))\n",
    "ys = mlp(xs)\n",
    "\n",
    "plt.plot(xs,ys)\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-0.05,0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron (MLP)\n",
    "\n",
    "### Aktivierungsfunktionen\n",
    "Wie erwähnt gibt es verschiedene Aktivierungsfunktionen. Diese kommen je nach Aufgabe oder Layer zum Einsatz:\n",
    "- Layer\n",
    "  - Hidden: Bei MLPs kommt in der Regel die im Hidden Layer eine ReLU zum Einsatz (es können aber auch andere verwandte Funktionen versucht werden wie: Leaky ReLU / Parametric ReLU, ELU oder SeLU)\n",
    "  - Output: \n",
    "    - Regression: Hier wird häufig eine lineare Funktion bzw die Identität verwendet \n",
    "    - Classification:\n",
    "      - Binary oder Multi-Label Classification: Also Funktion sollte im Output Sigmoid verwendet werden\n",
    "      - Multi-Class Classification: Also Funktion sollte im Output Softmax verwendet werden\n",
    "\n",
    "Zusätzlich sollte erwähnt werden das es noch die TanH Funktion gibt. Zum nachlesen findet ihr [hier](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/) mehr.\n",
    "\n",
    "\n",
    "### Gewichts Initialisierung\n",
    "Um Probleme beim Lernen zu vermeiden kann es sinnvoll sein die lernbaren gewichte mit Werten zu initialisieren. Ein Beispiel hierfür ist das Verwenden von He Initialisierung der Gewichte und Biases für ReLU-Aktivierungsfunktion. Weitere Initialisierung lassen sich im [hier](https://www.tensorflow.org/api_docs/python/tf/keras/initializers) und im [Paper](https://arxiv.org/ftp/arxiv/papers/2102/2102.07004.pdf) finden.\n",
    "\n",
    "In der nächsten Zelle erstellen wir das neuronale Netz. Dafür verwenden wir ein [sequentielles Modell](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?hl=en). Diese hilft dabei ein Modell aus einer Liste von Layern zu erstellen und die Layer zu trainieren. Als Layer nehmen wir das [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?hl=en)-Layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "y = to_categorical(iris.target)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.initializers import HeNormal\n",
    "\n",
    "model = Sequential(name=\"classification_mlp\")\n",
    "model.add(Dense(8, activation='relu', input_dim=4, bias_initializer=HeNormal(), kernel_initializer=HeNormal()))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wir gezeigt, wie wir die Struktur unseres neuronalen Netzes ausgeben lassen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "\n",
    "utils.plot_model(\n",
    "    model, \n",
    "    show_shapes=True, \n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    show_layer_activations=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Optimizer und Metriken\n",
    "\n",
    "Folgend sind die Dokumentationen für die Loss-Funktionen, Optimiere und Metriken verlinkt. Als kleine Empfehlung könnt ihr meistens gut auf adaptiven Optimierer zurückgreifen wie `Adam`.\n",
    "* [Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    "* [Optimizer](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "* [Metriken](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import (\n",
    "    CategoricalAccuracy,\n",
    "    Recall,\n",
    "    Precision,\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.1),\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        CategoricalAccuracy(),\n",
    "        Recall(),\n",
    "        Precision(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einrichten von TensorBoard\n",
    "Um die Visualisierung des Trainings zu automatisieren, kann [TensorBoard](https://www.tensorflow.org/tensorboard) verwendet werden. Dieses hilft dabei den Trainingsfortschritt zu visualisieren. Um TensorBoard im Training zu aktivieren müssen wir uns eine callback-Funktion erstellen ([Doku](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard?hl=en)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=f\"logs/fit/{datetime.now().strftime('%Y%m%d-%H%M%S')}\", \n",
    "    histogram_freq=1,\n",
    "    write_graph=True,\n",
    "    write_images=True,\n",
    "    update_freq='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainieren des neuronalen Netzes\n",
    "Für das Training müssen wir nicht unbedingt eine Trainingschleife schreiben. Stattdessen können wir auch die `fit()`-Methode des `Sequential`-Modells verwenden.\n",
    "Wenn der Loss sich beim Lernen komisch verhält, ist das ein Anzeichen, das noch nicht optimal gelernt wird. Weiter Infos sind [hier](https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic?hl=de) zu finden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=x_train, \n",
    "    y=y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=15,\n",
    "    batch_size=25,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, rec, prec = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --host 127.0.0.1 --port=8080 --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test_classes, y_pred_classes, display_labels=iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test.ravel(), y_pred.ravel())\n",
    "    \n",
    "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc(fpr, tpr)))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Wahlpflichtach Künstliche Intelligenz II: Praktikum "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
